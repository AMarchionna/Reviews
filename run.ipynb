{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis with KNN and PCA\n",
    "\n",
    "In this notebook, we predict wether a given review is either positive or negative, using a model that uses the techniques of KNN and PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid search for best parameters.\n",
    "from reviews.evaluate import Evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this, we are given first a _train dataset_ to train our model. In this dataset, each line of the input consists of a formatted review (that is, with no capital letters, no stops or commas, as we are just interested in the *words*). In this way, we are able to identify which words are the ones that appear the most in the train set of reviews. We exclude the words that appear the most, because they usually don't give us information about the review as they are pronouns. Let $\\{\\mathcal{D}_i\\}_i$ be the dictionary of words chosen. We transform each review into a vector $(x_i)_i$ where each $x_i$ denotes the number of times that the word $\\mathcal{D_i}$ appears in the $i-th$ review. Let $X$ be the matrix obtained by placing in the $i-th$ row the vector corresponding to the $i-th$ review. We also have a vector that contains to which category each one of the reviews corresponds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_TRAIN = 3000  # Max train reviews.\n",
    "MAX_TEST = 500    # Max test reviews.\n",
    "path = \"data/imdb_small.csv\"  # Path to read data.\n",
    "\n",
    "# Set the model.\n",
    "model = Evaluate(MAX_TRAIN, MAX_TEST)\n",
    "model.read(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Principal Component Analysis (PCA) technique obtains the main directions in which the data in $X$ is distributed in space. In this way, we can reduce the dimension of the space we are looking at by just multiplying the $X$ matrix by the main directions, which happen to be the eigenvectors of the covariant matrix associated to $X$. The number of directions will be the *components* variable in the PCA class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "components = 55  # Number of components for PCA. \n",
    "# We hardcoded a good value of this variable.\n",
    "model.set_PCA(components)  # Set PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The $k$ Nearest Neighbours (KNN) technique predicts a classification to a test review vector in the following way:\n",
    "we first normalize each vector in the train set, and the test review vector we are looking that, and we then find the $k$ vectors in the train data set that are closest in euclidean norm to the test review vector. We then count how many of them belong to each of the categories (in this case, positive and negative), and we establish our prediction to be the category that appears the most in those $k$ vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neighbours = 221  # Number of neighbours for KNN.\n",
    "# We hardcoded a good value of this variable.\n",
    "model.set_KNN(neighbours)  # Set KNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute the accuracy of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.score()  # Compute accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now set up a grid search to find the best values for our meta parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc = 0  # For the best accuracy.\n",
    "best_val = []  # Best parameters.\n",
    "\n",
    "for components in range(20,400,5):  # Vary the components.\n",
    "    model.set_PCA(components)  # Set PCA in the model.\n",
    "    for neighbours in range(20,301,5):  # Vary the neighbours.\n",
    "        model.set_KNN(neighbours)  # Set KNN in the model.\n",
    "        score = model.score()  # Calculate the score of the model.\n",
    "        if score == best_acc:  # If the actual score is equal to the previous best.\n",
    "            mejores.append((components, neighbours))  # Add the parameters to best parameters.\n",
    "        elif score > best_acc:  # If the actual score is the best so far.\n",
    "            best_acc = score\n",
    "            best_val = [(components, neighbours)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The best accuracy found was {:.4f}\".format(best_acc), \"and the best parameters were:\")\n",
    "print(best_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
